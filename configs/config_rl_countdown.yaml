# @package _global_

defaults:
  - tokenizer: qwen2.5
  - training: default
  - data: countdown
  - _self_

# Override defaults here
experiment_suffix: qwen2.5-0.5b-instruct-rl-grpo

seed: 42

tokenizer:
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"

data:
  batch_size: 2  # Increased to 2 to utilize more GPU memory
  train_file: "countdown_hf_train.jsonl" # Full HF dataset (327k)
  val_file: "countdown_hf_val.jsonl"     # TinyZero's test split (1k)
  test_file: "countdown_cd3_test.jsonl"  # SPG's test set (256) - Keep for benchmarking
  num_workers: 0 # Set to 0 to avoid pickling model to workers (CPU OOM fix)
  max_val_samples: 100
  max_test_samples: 100

# RL Module Configuration
# We wrap the base model in the RL module
rl_module:
  _target_: tzd.rl.module.RLDiffusionModule
  lr: 1e-6
  num_generations: 4
  beta: 0.01
  use_ref_model: true # Critical for stability
  generation_kwargs:
    num_steps: 64
    temperature: 1.0
  elbo_samples: 1 # Reduce ELBO samples to 1 to save memory


training:
  save_dir: ${hydra:runtime.output_dir}/checkpoints
  epochs: 10
  gradient_accumulation_steps: 8 # Reduced to 8 to maintain effective batch size of 16
  val_generation_freq: 1
  precision: "bf16-mixed" # Use BF16 mixed precision to save memory

# Override model target to be the RL module
# The RL module takes the base model as an argument
model:
  _target_: tzd.rl.module.RLDiffusionModule
  model:
    _target_: tzd.models.diffusion_pretrained.from_pretrained
    pretrained_model_name: "Qwen/Qwen2.5-0.5B-Instruct"
    tokenizer: ${tokenizer}
    model_alias: "qwen-0.5b-diffusion"
    lr: 1e-6 # Base model LR (can be same as RL module LR)
    block_size: 2048
    generation_block_size: 2048
  lr: 1e-6
  num_generations: 4
  beta: 0.01
  use_ref_model: true
  generation_kwargs:
    num_steps: 64
    temperature: 1.0
  elbo_samples: 1

logger:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: diffusion_countdown
  name: rl_grpo_baseline
